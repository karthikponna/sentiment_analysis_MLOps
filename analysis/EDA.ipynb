{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from analysis_src.basic_data_ingestion import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\"postgresql://postgres:3333@localhost:5432/NLP\")\n",
    "data_loader.load_data(\"customer_reviews\")\n",
    "df =data_loader.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>helpfulness_numerator</th>\n",
       "      <th>helpfulness_denominator</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>414001</td>\n",
       "      <td>B000G6RYNE</td>\n",
       "      <td>ACYR6O588USK</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>1200614400</td>\n",
       "      <td>These potato chips are excellent.There are no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>414002</td>\n",
       "      <td>B0025ULYKI</td>\n",
       "      <td>ACYR6O588USK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1259020800</td>\n",
       "      <td>I'm not a potato chip addict, but sometimes li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>414003</td>\n",
       "      <td>B003M8GSWQ</td>\n",
       "      <td>ACYR6O588USK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1318982400</td>\n",
       "      <td>These inexpensive little rewards for dogs seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414004</td>\n",
       "      <td>B001LGGH40</td>\n",
       "      <td>ACYR6O588USK</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1235433600</td>\n",
       "      <td>Though it is a bit expensive, this juice with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>414005</td>\n",
       "      <td>B0001FQVCA</td>\n",
       "      <td>ACYR6O588USK</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1203897600</td>\n",
       "      <td>I could eat it with a spoon, it's so good.&lt;br ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  product_id       user_id  helpfulness_numerator  \\\n",
       "0  414001  B000G6RYNE  ACYR6O588USK                     14   \n",
       "1  414002  B0025ULYKI  ACYR6O588USK                      1   \n",
       "2  414003  B003M8GSWQ  ACYR6O588USK                      0   \n",
       "3  414004  B001LGGH40  ACYR6O588USK                      1   \n",
       "4  414005  B0001FQVCA  ACYR6O588USK                     14   \n",
       "\n",
       "   helpfulness_denominator  score        time  \\\n",
       "0                       17      5  1200614400   \n",
       "1                        1      5  1259020800   \n",
       "2                        0      5  1318982400   \n",
       "3                        3      5  1235433600   \n",
       "4                       16      5  1203897600   \n",
       "\n",
       "                                         review_text  \n",
       "0  These potato chips are excellent.There are no ...  \n",
       "1  I'm not a potato chip addict, but sometimes li...  \n",
       "2  These inexpensive little rewards for dogs seem...  \n",
       "3  Though it is a bit expensive, this juice with ...  \n",
       "4  I could eat it with a spoon, it's so good.<br ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60962 entries, 0 to 60961\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   id                       60962 non-null  int64 \n",
      " 1   product_id               60962 non-null  object\n",
      " 2   user_id                  60962 non-null  object\n",
      " 3   helpfulness_numerator    60962 non-null  int64 \n",
      " 4   helpfulness_denominator  60962 non-null  int64 \n",
      " 5   score                    60962 non-null  int64 \n",
      " 6   time                     60962 non-null  int64 \n",
      " 7   review_text              60962 non-null  object\n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from abc import ABC, abstractmethod\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Abstract Base Class for Preprocessing Strategy\n",
    "# -----------------------------------------------\n",
    "# This class defines a common interface for different preprocessing strategies.\n",
    "# Subclasses must implement the preprocess method.\n",
    "class PreprocessingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def data_preprocessing(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Abstract method to preprocess the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The input DataFrame to be processed.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The processed DataFrame.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Concrete Strategy for Basic Preprocessing\n",
    "# ---------------------------------------------\n",
    "# This strategy implements basic preprocessing and text_preprocessing steps for customer reviews data.\n",
    "class BasicPreprocessingStrategy(PreprocessingStrategy):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the BasicPreprocessingStrategy with stop words and lemmatizer.\n",
    "\n",
    "        Attributes:\n",
    "            my_stopword (set): A set of English stop words.\n",
    "            my_lemmatizer (WordNetLemmatizer): An instance of the WordNetLemmatizer for lemmatization.\n",
    "        \"\"\"\n",
    "        self.my_stopword = set(stopwords.words('english'))\n",
    "        self.my_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def data_preprocessing(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocesses the customer reviews DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The input DataFrame to be processed.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The processed DataFrame.\n",
    "        \"\"\"\n",
    "        logging.info(\"Started basic preprocessing of the data.\")\n",
    "\n",
    "        df = df[['review_text', 'score']]\n",
    "\n",
    "        # Drop NA values and reset index\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        df['score'] = df['score'].astype(int)\n",
    "\n",
    "        # Remove neutral reviews (Score 3)\n",
    "        df = df[df['score'] != 3]\n",
    "\n",
    "        # Label the reviews\n",
    "        df['label'] = np.where(df['score'] >= 4, 1, 0)  # 1 for positive, 0 for negative\n",
    "\n",
    "        df = df.drop(columns=['score'])\n",
    "\n",
    "        #Apply text preprocessing \n",
    "        df[\"review_text\"] = df[\"review_text\"].apply(self.text_preprocessing)\n",
    "\n",
    "        logging.info(f\"Basic preprocessing completed. Number of records after preprocessing: {len(df)}.\")\n",
    "\n",
    "        return df \n",
    "    \n",
    "    def text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        Apply a series of text preprocessing steps to the given text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed text.\n",
    "        \"\"\"\n",
    "        text = self.lower_text(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.remove_urls(text)\n",
    "        text = self.replace_special_character_to_string_equivalent(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.remove_non_alpha(text)\n",
    "        text = self.remove_extra_spaces(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.text_lemmatization(text)\n",
    "        return text\n",
    "    \n",
    "    def lower_text(self, text):\n",
    "        \"\"\"\n",
    "        Converts the input text to lowercase.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to convert.\n",
    "\n",
    "        Returns:\n",
    "            str: The lowercase version of the input text.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"\n",
    "        Removes HTML tags from the input text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text potentially containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "            str: The text without HTML tags.\n",
    "        \"\"\"\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        return text\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        \"\"\"\n",
    "        Removes URLs from the input text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text potentially containing URLs.\n",
    "\n",
    "        Returns:\n",
    "            str: The text without URLs.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"http\\S+\",\"\", text)\n",
    "        return text\n",
    "    \n",
    "    def replace_special_character_to_string_equivalent(self,text):\n",
    "        \"\"\"\n",
    "        Replaces special characters in the input text with their string equivalents.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text containing special characters.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with special characters replaced by their equivalents.\n",
    "        \"\"\"\n",
    "        replacements = {\n",
    "            '%':\"percent\", \n",
    "            '$':\"dollar\",\n",
    "            '₹':\"rupee\",\n",
    "            '€':\"euro\",\n",
    "            '@':\"at\",\n",
    "        }\n",
    "        for char,word in replacements.items():\n",
    "            text = text.replace(char,word)\n",
    "        return text\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"\n",
    "        Expands contractions in the input text to their full form.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text containing contractions.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with contractions expanded.\n",
    "        \"\"\"\n",
    "        contractions = {\n",
    "            \"won't\":\"will not\",\n",
    "            \"can't\":\"cannot\",\n",
    "            \"n't\":\"not\",\n",
    "            \"'re\":\"are\",\n",
    "            \"'s\":\"is\",\n",
    "            \"'d\":\"would\",\n",
    "            \"'ll\":\"will\",\n",
    "            \"'t\":\"not\",\n",
    "            \"'ve\":\"have\",\n",
    "            \"'m\":\"am\",\n",
    "        }\n",
    "        for contraction, expand in contractions.items():\n",
    "            text = re.sub(contraction, expand, text)\n",
    "        return text\n",
    "    \n",
    "    def remove_non_alpha(self,text):\n",
    "        \"\"\"\n",
    "        Removes non-alphabetical characters from the input text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with non-alphabetical characters removed.\n",
    "        \"\"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [re.sub('[^A-Za-z]','',word) for word in words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def remove_extra_spaces(self,text):\n",
    "        \"\"\"\n",
    "        Removes extra spaces from the input text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with extra spaces removed.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'\\s+',' ', text).strip()\n",
    "        return text \n",
    "    \n",
    "    def remove_stopwords(self,text):\n",
    "        \"\"\"\n",
    "        Removes stopwords from the input text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text from which to remove stopwords.\n",
    "\n",
    "        Returns:\n",
    "            str: The text with stopwords removed.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.my_stopword]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def text_lemmatization(self, text):\n",
    "        \"\"\"\n",
    "        Lemmatizes the words in the input text.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to lemmatize.\n",
    "\n",
    "        Returns:\n",
    "            str: The lemmatized text.\n",
    "        \"\"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        lemmatized_words = [self.my_lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "    \n",
    "\n",
    "    \n",
    "# Context Class for Data Preprocessing\n",
    "# --------------------------------------\n",
    "# This class uses a PreprocessingStrategy to preprocess the data.\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, strategy:PreprocessingStrategy):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessor with the DataFrame and a strategy.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame containing customer reviews data.\n",
    "            strategy (PreprocessingStrategy): The strategy for preprocessing.\n",
    "        \"\"\"\n",
    "        self._strategy=strategy\n",
    "\n",
    "    def set_strategy(self, strategy:PreprocessingStrategy):\n",
    "        \"\"\"\n",
    "        Sets a new strategy for the DataPreprocessor.\n",
    "\n",
    "        Parameters:\n",
    "            strategy (PreprocessingStrategy): The new strategy to be used for preprocessing.\n",
    "        \"\"\"\n",
    "        logging.info(\"Switching preprocessing strategy\")\n",
    "        self._strategy=strategy\n",
    "\n",
    "    def preprocess(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the preprocessing using the current strategy.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The processed DataFrame.\n",
    "        \"\"\"\n",
    "        logging.info(\"Preprocessing data using the selected strategy.\")\n",
    "        return self._strategy.data_preprocessing(df)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # # Example DataFrame (replace with actual data loading)\n",
    "    # df = pd.DataFrame({\n",
    "    #     'review_text': ['Good product! Highly recommend.', 'Just okay.', 'Worst product ever!'],\n",
    "    #     'score': [5, 4, 1],\n",
    "    #     'label': [1, 1, 0]\n",
    "    # })\n",
    "\n",
    "    # # Initialize data preprocessor with a specific strategy\n",
    "    # strategy = BasicPreprocessingStrategy()\n",
    "    # preprocessor = DataPreprocessor(strategy)\n",
    "    # preprocessed_data = preprocessor.preprocess(df)\n",
    "    # print(preprocessed_data)\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36057/271553141.py:128: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "strategy = BasicPreprocessingStrategy()\n",
    "df_preprocessor = DataPreprocessor(strategy)\n",
    "df_pre = df_preprocessor.preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 56208 entries, 0 to 60961\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_text  56208 non-null  object\n",
      " 1   label        56208 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "class DataSampler:\n",
    "    \"\"\"A class to sample customer reviews data for model training.\n",
    "    \n",
    "    Attributes:\n",
    "        df: pd.DataFrame\n",
    "            The DataFrame containing labeled customer reviews data.\n",
    "    \n",
    "    Methods:\n",
    "        sample_data() -> pd.DataFrame:\n",
    "            Samples 100,000 reviews (50,000 positive and 50,000 negative).\n",
    "    \"\"\"\n",
    "    def __init__(self, df:pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the DataSampler with the DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            df: pd.DataFrame\n",
    "                The DataFrame containing labeled customer reviews data.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def sample_data(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Samples 100,000 reviews by shuffling the DataFrame \n",
    "        and selecting 50,000 positive and 50,000 negative reviews.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the sampled reviews.\n",
    "        \"\"\"\n",
    "        logging.info(\"Started sampling data for model training.\")\n",
    "\n",
    "        #Shuffle the data\n",
    "        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Select 50,000 negative reviews (label 0)\n",
    "        negative_reviews = df_shuffled[df_shuffled[\"label\"]==0][:5000]\n",
    "\n",
    "        #select 50,000 positive reviews (label 1)\n",
    "        positive_reviews = df_shuffled[df_shuffled[\"label\"]==1][:5000]\n",
    "\n",
    "        # Combine the selected reviews\n",
    "        sampled_data = pd.concat([negative_reviews, positive_reviews], ignore_index=True)\n",
    "\n",
    "        logging.info(f\"Sampling completed. Number of records after sampling: {len(sampled_data)}.\")\n",
    "\n",
    "        return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DataSampler(df_pre)\n",
    "\n",
    "sampled_df = sampler.sample_data(df_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_text  10000 non-null  object\n",
      " 1   label        10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "sampled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    5000\n",
       "1    5000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Abstract Base Class for Data Splitting Strategy\n",
    "# -----------------------------------------------\n",
    "# This class defines a common interface for different data splitting strategies.\n",
    "# Subclasses must implement the split_data method.\n",
    "class DataSplittingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def split_data(self, df:pd.DataFrame, target_column:str):\n",
    "        \"\"\"\n",
    "        Abstract method to split the data into training and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame to be split.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "        Returns:\n",
    "        X_train, X_test, y_train, y_test: The training and testing splits for features and target.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Concrete Strategy for Simple Train-Test Split\n",
    "# ---------------------------------------------\n",
    "# This strategy implements a simple train-test split.\n",
    "class SimpleTrainTestSplitStrategy(DataSplittingStrategy):\n",
    "    def __init__(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleTrainTestSplitStrategy with specific parameters.\n",
    "\n",
    "        Parameters:\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "        random_state (int): The seed used by the random number generator.\n",
    "        \"\"\"\n",
    "        self.test_size=test_size\n",
    "        self.random_state=random_state\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, target_column: str):\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets using a simple train-test split.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame to be split.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "        Returns:\n",
    "        X_train, X_test, y_train, y_test: The training and testing splits for features and target.\n",
    "        \"\"\"\n",
    "        logging.info(\"Performing simple train-test split.\")\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state= self.random_state\n",
    "        )\n",
    "\n",
    "        logging.info(\"Train-test split completed.\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "# Context Class for Data Splitting\n",
    "# --------------------------------\n",
    "# This class uses a DataSplittingStrategy to split the data.\n",
    "class DataSplitter:\n",
    "    def __init__(self, strategy:DataSplittingStrategy):\n",
    "        \"\"\"\n",
    "        Initializes the DataSplitter with a specific data splitting strategy.\n",
    "\n",
    "        Parameters:\n",
    "        strategy (DataSplittingStrategy): The strategy to be used for data splitting.\n",
    "        \"\"\"\n",
    "        self._strategy = strategy\n",
    "\n",
    "    def set_strategy(self, strategy:DataSplittingStrategy):\n",
    "        \"\"\"\n",
    "        Sets a new strategy for the DataSplitter.\n",
    "\n",
    "        Parameters:\n",
    "        strategy (DataSplittingStrategy): The new strategy to be used for data splitting.\n",
    "        \"\"\"\n",
    "        logging.info(\"Switching data splitting strategy\")\n",
    "        self._strategy = strategy\n",
    "\n",
    "    def split(self, df:pd.DataFrame, target_column:str):\n",
    "        \"\"\"\n",
    "        Executes the data splitting using the current strategy.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame to be split.\n",
    "        target_column (str): The name of the target column.\n",
    "\n",
    "        Returns:\n",
    "        X_train, X_test, y_train, y_test: The training and testing splits for features and target.\n",
    "        \"\"\"\n",
    "        logging.info(\"Splitting data using the selected strategy.\")\n",
    "        return self._strategy.split_data(df, target_column)\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataframe (replace with actual data loading)\n",
    "    # df = pd.read_csv('your_data.csv')\n",
    "\n",
    "    # Initialize data splitter with a specific strategy\n",
    "    # data_splitter = DataSplitter(SimpleTrainTestSplitStrategy(test_size=0.2, random_state=42))\n",
    "    # X_train, X_test, y_train, y_test = data_splitter.split(df, target_column='SalePrice')\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DataSplitter(strategy=SimpleTrainTestSplitStrategy())\n",
    "X_train, X_test, y_train, y_test = splitter.split(sampled_df, target_column=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TfidfVectorization:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the TfidfVectorizer for use on text data.\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_transform(self, X_train:pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the training data and transforms it.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data to fit and transform.\n",
    "\n",
    "        Returns:\n",
    "            sparse matrix: The transformed training data in sparse matrix form.\n",
    "        \"\"\"\n",
    "        logging.info(\"TF-IDF Vectorizer: Fitting and transforming training data.\")\n",
    "        return self.vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    def transform(self, X_test:pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Transforms the test data using the already-fitted vectorizer.\n",
    "\n",
    "        Parameters:\n",
    "            X_test (pd.Series): The test data to transform.\n",
    "\n",
    "        Returns:\n",
    "            sparse matrix: The transformed test data in sparse matrix form.\n",
    "        \"\"\"\n",
    "        logging.info(\"TF-IDF Vectorizer: Transforming test data\")\n",
    "        return self.vectorizer.transform(X_test)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # # Example DataFrame (replace with actual data loading)\n",
    "    # df_train = pd.DataFrame({\n",
    "    #     'review_text': ['Good product! Highly recommend.', 'Just okay.', 'Worst product ever!']\n",
    "    # })\n",
    "    # df_test = pd.DataFrame({\n",
    "    #     'review_text': ['Amazing quality!', 'Not good at all.']\n",
    "    # })\n",
    "\n",
    "    # # Initialize the TF-IDF Vectorizer\n",
    "    # tfidf_vectorizer = TfidfVectorization()\n",
    "    # tf_x_train = tfidf_vectorizer.fit_transform(df_train['review_text'])\n",
    "    # tf_x_test = tfidf_vectorizer.transform(df_test['review_text'])\n",
    "\n",
    "    # print(\"TF-IDF Vectors for Training Data:\\n\", tf_x_train.toarray())\n",
    "    # print(\"TF-IDF Vectors for Test Data:\\n\", tf_x_test.toarray())\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorization()\n",
    "tf_X_train = vectorizer.fit_transform(X_train[\"review_text\"])\n",
    "tf_X_test = vectorizer.transform(X_test[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 271128 stored elements and shape (8000, 20842)>\n",
      "  Coords\tValues\n",
      "  (0, 10543)\t0.12939230732465548\n",
      "  (0, 8067)\t0.28092700382345537\n",
      "  (0, 1621)\t0.16615664095950014\n",
      "  (0, 4518)\t0.47103293355055575\n",
      "  (0, 18516)\t0.1741517281248534\n",
      "  (0, 8731)\t0.18951420280216913\n",
      "  (0, 3573)\t0.3275615309254489\n",
      "  (0, 17443)\t0.33489521610541567\n",
      "  (0, 4911)\t0.18046328901902722\n",
      "  (0, 20010)\t0.18304516012964425\n",
      "  (0, 167)\t0.2013097435448063\n",
      "  (0, 7677)\t0.17559395337410352\n",
      "  (0, 13551)\t0.2954780586148099\n",
      "  (0, 12731)\t0.18557102166885456\n",
      "  (0, 588)\t0.15462024507937902\n",
      "  (0, 7166)\t0.20076278820103954\n",
      "  (0, 16293)\t0.21394717235785957\n",
      "  (1, 18516)\t0.07060157118168427\n",
      "  (1, 4911)\t0.073160294661121\n",
      "  (1, 20010)\t0.07420699203794795\n",
      "  (1, 167)\t0.08161150246097915\n",
      "  (1, 12731)\t0.1504619659727828\n",
      "  (1, 7166)\t0.162779530636036\n",
      "  (1, 16293)\t0.1734694990510156\n",
      "  (1, 7783)\t0.0708917589850204\n",
      "  :\t:\n",
      "  (7998, 16821)\t0.20236463059911358\n",
      "  (7998, 3329)\t0.20449793444398334\n",
      "  (7998, 15168)\t0.233655451646146\n",
      "  (7998, 6659)\t0.29830608875841225\n",
      "  (7998, 11492)\t0.3017657778029553\n",
      "  (7998, 4473)\t0.33311589989178003\n",
      "  (7998, 8277)\t0.34198838676133303\n",
      "  (7998, 1325)\t0.6394746250069774\n",
      "  (7999, 10307)\t0.09276969127781311\n",
      "  (7999, 7510)\t0.1234639189050649\n",
      "  (7999, 9505)\t0.12688083764921274\n",
      "  (7999, 10402)\t0.1419069115057694\n",
      "  (7999, 16411)\t0.22079834301318366\n",
      "  (7999, 18134)\t0.18972795773532344\n",
      "  (7999, 16661)\t0.17670869246280446\n",
      "  (7999, 17552)\t0.24314402490109588\n",
      "  (7999, 2962)\t0.42485847721080433\n",
      "  (7999, 14134)\t0.17937985659403052\n",
      "  (7999, 16729)\t0.18133871195157242\n",
      "  (7999, 20776)\t0.27160623636497183\n",
      "  (7999, 4265)\t0.22999393327147927\n",
      "  (7999, 5131)\t0.24800173682097962\n",
      "  (7999, 12871)\t0.28392397753490417\n",
      "  (7999, 4277)\t0.3475081679485155\n",
      "  (7999, 2721)\t0.3601502245453784\n"
     ]
    }
   ],
   "source": [
    "print(tf_X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Abstract Base Class for Model Building Strategy\n",
    "class ModelBuildingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def build_and_train_model(self, X_train:pd.DataFrame, y_train:pd.Series, fine_tuning:bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Abstract method to build and train a model.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Flag to indicate if fine-tuning should be performed.\n",
    "\n",
    "        Returns:\n",
    "            Any: A trained scikit-learn model instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Concrete Strategy for Logistic Regression\n",
    "class LogisticRegressionStrategy(ModelBuildingStrategy):\n",
    "    def build_and_train_model(self, X_train: pd.DataFrame, y_train: pd.Series, fine_tuning: bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Trains a Logistic Regression model on the provided training data.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Not applicable for Logistic Regression, defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            LogisticRegression: A trained Logistic Regression model.\n",
    "        \"\"\"\n",
    "        logging.info(\"Training the Logistic Regression Model.\")\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train,y_train)\n",
    "        joblib.dump(model, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/logistic.pkl\")\n",
    "        logging.info(\"Logistic Regression training completed.\")\n",
    "        return model\n",
    "    \n",
    "# Concrete Strategy for XGBoost\n",
    "class XGBoostStrategy(ModelBuildingStrategy):\n",
    "    def build_and_train_model(self, X_train: pd.DataFrame, y_train: pd.Series, fine_tuning: bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Trains an XGBoost model on the provided training data, optionally with fine-tuning.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Flag to indicate if fine-tuning should be performed.\n",
    "\n",
    "        Returns:\n",
    "            XGBClassifier: A trained XGBoost model (either fine-tuned or default).\n",
    "        \"\"\"\n",
    "        if fine_tuning:\n",
    "            logging.info(\"Started fine-tuning the XGBoost model.\")\n",
    "            params = {\n",
    "                \"n_estimators\": [50, 100, 150, 200],\n",
    "                \"max_depth\": [2, 4, 6, 8],\n",
    "                \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "                \"subsample\": [0.5, 0.7, 0.8, 1.0],\n",
    "                \"colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "            }\n",
    "            \n",
    "            xgb_model = XGBClassifier()\n",
    "            clf = RandomizedSearchCV(xgb_model, params, cv=5, n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "            joblib.dump(clf, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/xgb_fine_tuned.pkl\")\n",
    "            logging.info(\"Finished Hyperparameter search for XGBoost.\")\n",
    "            return clf\n",
    "        \n",
    "        else:\n",
    "            logging.info(\"Started training the XGBoost model.\")\n",
    "            model = XGBClassifier(\n",
    "                learning_rate=0.3,\n",
    "                max_depth=8,\n",
    "                min_child_weight=1,\n",
    "                n_estimators=50,\n",
    "                random_state=0,\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            joblib.dump(model, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/xgb.pkl\")\n",
    "            logging.info(\"Completed training the XGBoost model.\")\n",
    "            return model\n",
    "        \n",
    "# Concrete Strategy for SVM\n",
    "class SVCStrategy(ModelBuildingStrategy):\n",
    "    def build_and_train_model(self, X_train: pd.DataFrame, y_train: pd.Series, fine_tuning: bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Trains a Support Vector Classifier model on the provided training data, optionally with fine-tuning.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Flag to indicate if fine-tuning should be performed.\n",
    "\n",
    "        Returns:\n",
    "            SVC: A trained SVC model (either fine-tuned or default).\n",
    "        \"\"\"\n",
    "        if fine_tuning:\n",
    "            logging.info(\"Started fine-tuning the SVM model.\")\n",
    "            params = {\n",
    "                \"C\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                \"gamma\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "            }\n",
    "            svm = SVC()\n",
    "            clf = RandomizedSearchCV(svm, params, cv=5, n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "            joblib.dump(clf, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/svm_fine_tuned.pkl\")\n",
    "            logging.info(\"Finished Hyperparameter search for SVM.\")\n",
    "            return clf\n",
    "        \n",
    "        else:\n",
    "            logging.info(\"Started training the SVM model.\")\n",
    "            model = SVC(C=1.0, gamma='scale')\n",
    "            model.fit(X_train, y_train)\n",
    "            joblib.dump(model, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/svm.pkl\")\n",
    "            logging.info(\"Completed training the SVM model.\")\n",
    "            return model\n",
    "            \n",
    "# Concrete Strategy for Naive Bayes\n",
    "class NaiveBayesStrategy(ModelBuildingStrategy):\n",
    "    def build_and_train_model(self, X_train: pd.DataFrame, y_train: pd.Series, fine_tuning: bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Trains a Naive Bayes model on the provided training data.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Not applicable for Naive Bayes, defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            MultinomialNB: A trained Naive Bayes model.\n",
    "        \"\"\"\n",
    "        logging.info(\"Training the Naive Bayes model.\")\n",
    "        model = MultinomialNB()\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/NBayes.pkl\")\n",
    "        logging.info(\"Completed training the Naive Bayes model.\")\n",
    "        return model\n",
    "    \n",
    "# Concrete Strategy for Random Forest\n",
    "class RandomForestStrategy(ModelBuildingStrategy):\n",
    "    def build_and_train_model(self, X_train: pd.DataFrame, y_train: pd.Series, fine_tuning: bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Trains a Random Forest model on the provided training data.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Not applicable for Random Forest, defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            RandomForestClassifier: A trained Random Forest model.\n",
    "        \"\"\"\n",
    "        logging.info(\"Training the Random Forest model.\")\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, \"/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/saved_models/rf.pkl\")\n",
    "        logging.info(\"Completed training the Random Forest model.\")\n",
    "        return model\n",
    "    \n",
    "# Context Class for Model Building Strategy\n",
    "class ModelBuilder:\n",
    "    def __init__(self, strategy:ModelBuildingStrategy):\n",
    "        \"\"\"\n",
    "        Initializes the ModelBuildingStrategy with the X_train, y_train, fine_tuning and a strategy.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Flag to indicate if fine-tuning should be performed.\n",
    "        \"\"\"\n",
    "        self._strategy = strategy\n",
    "\n",
    "    def set_strategy(self, strategy:ModelBuildingStrategy):\n",
    "        \"\"\"\n",
    "        Set the model building strategy.\n",
    "\n",
    "        Parameters:\n",
    "            strategy (ModelBuildingStrategy): The strategy to set.\n",
    "        \"\"\"\n",
    "        self._strategy = strategy\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series, fine_tuning: bool = False) -> Any:\n",
    "        \"\"\"\n",
    "        Train the model using the set strategy.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training data features.\n",
    "            y_train (pd.Series): The training data labels/target.\n",
    "            fine_tuning (bool): Flag to indicate if fine-tuning should be performed.\n",
    "\n",
    "        Returns:\n",
    "            Any: A trained model instance from the chosen strategy.\n",
    "        \"\"\"\n",
    "        return self._strategy.build_and_train_model(X_train, y_train, fine_tuning)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # import numpy as np\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    # from sklearn.datasets import make_classification\n",
    "\n",
    "    # # Configure logging\n",
    "    # logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # # Generate synthetic data for demonstration purposes\n",
    "    # X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # # Initialize the ModelBuilder with different strategies\n",
    "    # # Logistic Regression\n",
    "    # logging.info(\"Training Logistic Regression Model\")\n",
    "    # logistic_strategy = LogisticRegressionStrategy()\n",
    "    # logistic_builder = ModelBuilder(logistic_strategy)\n",
    "    # logistic_model = logistic_builder.train(X_train, y_train)\n",
    "    # logging.info(\"Logistic Regression Model Trained and Saved.\")\n",
    "\n",
    "    # # XGBoost with fine-tuning\n",
    "    # logging.info(\"Training XGBoost Model with Fine-Tuning\")\n",
    "    # xgb_strategy = XGBoostStrategy()\n",
    "    # xgb_builder = ModelBuilder(xgb_strategy)\n",
    "    # xgb_model = xgb_builder.train(X_train, y_train, fine_tuning=True)\n",
    "    # logging.info(\"XGBoost Model (Fine-Tuned) Trained and Saved.\")\n",
    "\n",
    "    # # SVM with fine-tuning\n",
    "    # logging.info(\"Training SVM Model with Fine-Tuning\")\n",
    "    # svm_strategy = SVCStrategy()\n",
    "    # svm_builder = ModelBuilder(svm_strategy)\n",
    "    # svm_model = svm_builder.train(X_train, y_train, fine_tuning=True)\n",
    "    # logging.info(\"SVM Model (Fine-Tuned) Trained and Saved.\")\n",
    "\n",
    "    # # Naive Bayes\n",
    "    # logging.info(\"Training Naive Bayes Model\")\n",
    "    # nb_strategy = NaiveBayesStrategy()\n",
    "    # nb_builder = ModelBuilder(nb_strategy)\n",
    "    # nb_model = nb_builder.train(X_train, y_train)\n",
    "    # logging.info(\"Naive Bayes Model Trained and Saved.\")\n",
    "\n",
    "    # # Random Forest\n",
    "    # logging.info(\"Training Random Forest Model\")\n",
    "    # rf_strategy = RandomForestStrategy()\n",
    "    # rf_builder = ModelBuilder(rf_strategy)\n",
    "    # rf_model = rf_builder.train(X_train, y_train)\n",
    "    # logging.info(\"Random Forest Model Trained and Saved.\")\n",
    "\n",
    "    # # Example of using models on test data (Optional)\n",
    "    # test_data_sample = X_test[:5]  # Take first 5 examples for testing\n",
    "    # logging.info(\"Logistic Regression Prediction: %s\", logistic_model.predict(test_data_sample))\n",
    "    # logging.info(\"XGBoost Prediction: %s\", xgb_model.predict(test_data_sample))\n",
    "    # logging.info(\"SVM Prediction: %s\", svm_model.predict(test_data_sample))\n",
    "    # logging.info(\"Naive Bayes Prediction: %s\", nb_model.predict(test_data_sample))\n",
    "    # logging.info(\"Random Forest Prediction: %s\", rf_model.predict(test_data_sample))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List, Annotated\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from zenml import ArtifactConfig, step\n",
    "from zenml.client import Client\n",
    "\n",
    "# Get the active experiment tracker from ZenML\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "from zenml import Model\n",
    "\n",
    "model = Model(\n",
    "    name=\"customer_reviews_predictor\",\n",
    "    version=None,\n",
    "    license=\"Apache 2.0\",\n",
    "    description=\"Reviews predictor model for customer reviews\",\n",
    ")\n",
    "\n",
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name, model=model)\n",
    "def model_building_step(\n",
    "    X_train:csr_matrix, y_train:pd.Series, method:str, fine_tuning:bool = False\n",
    ") -> Annotated[ClassifierMixin, ArtifactConfig(name=\"trained_model\", is_model_artifact=True)]:\n",
    "    \"\"\"\n",
    "    Model building step using ZenML with multiple model options and MLflow tracking.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): The training data features.\n",
    "        y_train (pd.Series): The training data labels/target.\n",
    "        method (str): Model selection method, e.g., 'logistic_regression', 'xgboost', 'svm', 'naive_bayes', 'random_forest'.\n",
    "        fine_tuning (bool): Flag to indicate if fine-tuning should be performed, only applicable to certain models.\n",
    "\n",
    "    Returns:\n",
    "        Trained model instance.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Building model using method: {method}\")\n",
    "    \n",
    "    # Choose the appropriate strategy based on the method\n",
    "    if method == \"logistic_regression\":\n",
    "        strategy = LogisticRegressionStrategy()\n",
    "        logging.info(\"Selected Logistic Regression Strategy.\")\n",
    "    \n",
    "    elif method == \"xgboost\":\n",
    "        strategy = XGBoostStrategy()\n",
    "        logging.info(\"Selected XGBoost Strategy.\")\n",
    "\n",
    "    elif method == \"svc\":\n",
    "        strategy = SVCStrategy()\n",
    "        logging.info(\"Selected SVM Strategy.\")\n",
    "\n",
    "    elif method == \"naive_bayes\":\n",
    "        strategy = NaiveBayesStrategy()\n",
    "        logging.info(\"Selected Naive Bayes Strategy.\")\n",
    "\n",
    "    elif method == \"random_forest\":\n",
    "        strategy = RandomForestStrategy()\n",
    "        logging.info(\"Selected Random Forest Strategy.\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method '{method}' selected for model training.\")\n",
    "    \n",
    "    # Initialize ModelBuilder with the selected strategy\n",
    "    model_builder = ModelBuilder(strategy)\n",
    "\n",
    "    # Start an MLflow run to log the training process\n",
    "    if not mlflow.active_run():\n",
    "        mlflow.start_run()\n",
    "\n",
    "    try:\n",
    "        # Enable autologging to automatically log model parameters, metrics, and artifacts\n",
    "        mlflow.sklearn.autolog()\n",
    "\n",
    "        # Train the model with or without fine-tuning\n",
    "        logging.info(\"Started model training.\")\n",
    "        trained_model = model_builder.train(X_train, y_train, fine_tuning=fine_tuning)\n",
    "        logging.info(\"Model training completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during model training: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        #End the mlflow run\n",
    "        mlflow.end_run()\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRunning single step pipeline to execute step \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_5572bb0f-2d44-4858-89ff-f99d364d49a1'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact 1a0a7359-7333-479e-b0dc-8959c71d2217.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_3987cd67-778f-4f3e-91ef-6e35a0e90da2'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact c7ba3f5f-57e2-4a81-9a11-cb5a0f8578dc.\u001b[0m\n",
      "\u001b[1;35mExecuting a new run.\u001b[0m\n",
      "\u001b[1;35mCaching is disabled by default for \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mlocal-mlflow-stack\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  model_deployer: \u001b[0m\u001b[1;36mmlflow\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  experiment_tracker: \u001b[0m\u001b[1;36mmlflow_tracker\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mDashboard URL for Pipeline Run: \u001b[0m\u001b[34mhttp://127.0.0.1:8237/runs/d89a15c4-b68a-4d0f-bcfd-82824ca9eb53\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mCaching \u001b[0m\u001b[1;36mdisabled\u001b[1;35m explicitly for \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mNew model \u001b[0m\u001b[1;36mcustomer_reviews_predictor\u001b[1;35m was created implicitly.\u001b[0m\n",
      "\u001b[1;35mNew model version \u001b[0m\u001b[1;36m1\u001b[1;35m was created.\u001b[0m\n",
      "\u001b[1;35mModels can be viewed in the dashboard using ZenML Pro. Sign up for a free trial at \u001b[0m\u001b[34mhttps://www.zenml.io/pro/\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m has started.\u001b[0m\n",
      "2024/11/05 09:43:04 INFO mlflow.tracking.fluent: Experiment with name 'model_building_step' does not exist. Creating a new experiment.\n",
      "INFO:root:Building model using method: svc\n",
      "\u001b[1;35mBuilding model using method: svc\u001b[0m\n",
      "INFO:root:Selected SVM Strategy.\n",
      "\u001b[1;35mSelected SVM Strategy.\u001b[0m\n",
      "INFO:root:Started model training.\n",
      "\u001b[1;35mStarted model training.\u001b[0m\n",
      "INFO:root:Started training the SVM model.\n",
      "\u001b[1;35mStarted training the SVM model.\u001b[0m\n",
      "2024/11/05 09:43:13 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'toarray'\n",
      "INFO:root:Completed training the SVM model.\n",
      "\u001b[1;35mCompleted training the SVM model.\u001b[0m\n",
      "INFO:root:Model training completed.\n",
      "\u001b[1;35mModel training completed.\u001b[0m\n",
      "\u001b[33mExtracting step run metadata failed for component 'mlflow_tracker' of type 'experiment_tracker': 'NoneType' object has no attribute 'info'\u001b[0m\n",
      "/home/karthikponna/karthik/sentiment_analysis_mlops_project_1/sentiment_analysis_MLOps/mlops/lib/python3.12/site-packages/zenml/integrations/mlflow/experiment_trackers/mlflow_experiment_tracker.py:255: FutureWarning: ``mlflow.gluon.autolog`` is deprecated since 2.5.0. This method will be removed in a future release.\n",
      "  module.autolog(disable=True)\n",
      "\u001b[33mNo materializer is registered for type \u001b[0m\u001b[1;36m<class 'sklearn.svm._classes.SVC'>\u001b[33m, so the default Pickle materializer was used. Pickle is not production ready and should only be used for prototyping as the artifacts cannot be loaded when running with a different Python version. Please consider implementing a custom materializer for type \u001b[0m\u001b[1;36m<class 'sklearn.svm._classes.SVC'>\u001b[33m according to the instructions at \u001b[0m\u001b[34mhttps://docs.zenml.io/how-to/handle-data-artifacts/handle-custom-data-types\u001b[33m\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m has finished in \u001b[0m\u001b[1;36m3m15s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_building_step\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m3m17s\u001b[1;35m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = model_building_step(X_train=tf_X_train, y_train=y_train, method=\"svc\", fine_tuning=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Setup logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Abstract Base Class for Model Evaluation Strategy\n",
    "class ModelEvaluationStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def evaluate_model(\n",
    "        self, model:ClassifierMixin, X_test:pd.DataFrame, y_test:pd.Series\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Abstract method to evaluate a model.\n",
    "\n",
    "        Parameters:\n",
    "            model (ClassifierMixin): The trained model to evaluate.\n",
    "            X_test (pd.DataFrame): The testing data features.\n",
    "            y_test (pd.Series): The testing data labels/target.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Concrete Strategy for Classification Model Evaluation\n",
    "class ClassificationModelEvaluationStrategy(ModelEvaluationStrategy):\n",
    "    def evaluate_model(self, model: ClassifierMixin, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates a classification model using various metrics.\n",
    "\n",
    "        Parameters:\n",
    "            model (ClassifierMixin): The trained classification model to evaluate.\n",
    "            X_test (pd.DataFrame): The testing data features.\n",
    "            y_test (pd.Series): The testing data labels/target.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing accuracy, precision, recall, F1 score, ROC AUC, and confusion matrix.\n",
    "        \"\"\"\n",
    "        logging.info(\"Predicting using the trained model.\")\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        logging.info(\"Calculating evaluation metrics.\")\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        F1_score = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        Confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": F1_score,\n",
    "            \"ROC AUC\": roc_auc,\n",
    "            \"True Negatives\": Confusion_matrix[0][0],\n",
    "            \"False Positives\": Confusion_matrix[0][1],\n",
    "            \"False Negatives\": Confusion_matrix[1][0],\n",
    "            \"True Positives\": Confusion_matrix[1][1],\n",
    "\n",
    "        }\n",
    "        logging.info(f\"Model Evaluation Metrics: {metrics}\")\n",
    "        return metrics\n",
    "    \n",
    "# Context Class for Model Evaluation\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, strategy:ModelEvaluationStrategy):\n",
    "        \"\"\"\n",
    "        Initializes the ModelEvaluator with a specific model evaluation strategy.\n",
    "\n",
    "        Parameters:\n",
    "            strategy (ModelEvaluationStrategy): The strategy to be used for model evaluation.\n",
    "        \"\"\"\n",
    "        self._strategy = strategy\n",
    "\n",
    "    def set_strategy(self, strategy: ModelEvaluationStrategy):\n",
    "        \"\"\"\n",
    "        Sets a new strategy for the ModelEvaluator.\n",
    "\n",
    "        Parameters:\n",
    "            strategy (ModelEvaluationStrategy): The new strategy to be used for model evaluation.\n",
    "        \"\"\"\n",
    "        logging.info(\"Switching model evaluation strategy.\")\n",
    "        self._strategy = strategy\n",
    "\n",
    "    def evaluate(self, model: ClassifierMixin, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Executes the model evaluation using the current strategy.\n",
    "\n",
    "        Parameters:\n",
    "            model (ClassifierMixin): The trained model to evaluate.\n",
    "            X_test (pd.DataFrame): The testing data features.\n",
    "            y_test (pd.Series): The testing data labels/target.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        logging.info(\"Evaluating the model using the selected strategy.\")\n",
    "        return self._strategy.evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example trained model and data (replace with actual trained model and data)\n",
    "    # model = trained_sklearn_classification_model\n",
    "    # X_test = test_data_features\n",
    "    # y_test = test_data_target\n",
    "\n",
    "    # Initialize model evaluator with a specific strategy\n",
    "    # model_evaluator = ModelEvaluator(ClassificationModelEvaluationStrategy())\n",
    "    # evaluation_metrics = model_evaluator.evaluate(model, X_test, y_test)\n",
    "    # print(evaluation_metrics)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m  \u001b[38;5;66;03m# or another index/model depending on your logic\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example usage of ModelEvaluator\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluator(ClassificationModelEvaluationStrategy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "selected_model = model  # or another index/model depending on your logic\n",
    "\n",
    "# Example usage of ModelEvaluator\n",
    "model_evaluator = ModelEvaluator(ClassificationModelEvaluationStrategy())\n",
    "evaluation_metrics = model_evaluator.evaluate(selected_model, tf_X_test, y_test)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
